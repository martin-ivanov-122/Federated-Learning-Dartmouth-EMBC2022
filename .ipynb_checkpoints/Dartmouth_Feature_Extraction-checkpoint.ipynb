{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d55880a9",
   "metadata": {},
   "source": [
    "# Dartmouth Implemention of FL - EMBC 2022\n",
    "\n",
    "Main contributors: Binh Nguyen and Martin Ivanov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "688c3213",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import pywt\n",
    "import time\n",
    "import csv\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4e65e8d-1903-486f-8b4b-9aef01fb30fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sklearn toolbox libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.cluster import KMeans\n",
    "from skimage.restoration import denoise_wavelet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22ce0945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import utility functions and other functions\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612a90d6-5ef0-4d53-95a4-fb1ba8ee3cde",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "991a4ace-7cb9-42e8-a10d-68c6d0532ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import sensor values ->  (subject, sensor_num)\n",
    "from functions import sensor_val\n",
    "# Import EMA values ->  (subject, ema_num)\n",
    "from functions import ema_val\n",
    "# Kind of Useless\n",
    "from functions import activity_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b6c600a-46ba-429f-aa38-72c81a7aa4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject \n",
    "s1 =  [('u0'+str(x)) for x in range(9 + 1)]\n",
    "s2 =  ['u'+str(x) for x in range(10,57)]\n",
    "subject = s1+s2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7a5c81d-9246-4360-a2a7-eec6d599be09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHQ-9 Analysis\n",
    "\n",
    "from functions import survey_reader\n",
    "from functions import survey_encoder_phq\n",
    "\n",
    "surv_num = 0\n",
    "phq = survey_reader (surv_num)\n",
    "dataset_score_pre = []\n",
    "dataset_score_post = []\n",
    "user_pre = []\n",
    "user_post = []\n",
    "\n",
    "for i in range (len(phq)):\n",
    "    user = phq.iloc [i,2:-1]\n",
    "    phq_score = np.sum (survey_encoder_phq(user))\n",
    "    if (phq.iloc[i,1]=='pre'):\n",
    "        dataset_score_pre.append(phq_score)\n",
    "        user_pre.append (phq.iloc[i,0])\n",
    "    elif (phq.iloc[i,1]=='post'):\n",
    "        dataset_score_post.append(phq_score)\n",
    "        user_post.append (phq.iloc[i,0])\n",
    "        \n",
    "phq_post = np.vstack((user_post,dataset_score_post))\n",
    "phq_pre = np.vstack((user_pre,dataset_score_pre))\n",
    "\n",
    "phq_final = phq_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "738b2c4b-7826-406c-bf4a-518489f5960e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['u00' 'u01' 'u02' 'u03' 'u04' 'u05' 'u07' 'u09' 'u10' 'u14' 'u15' 'u16'\n",
      "  'u17' 'u18' 'u19' 'u20' 'u23' 'u24' 'u27' 'u30' 'u31' 'u32' 'u33' 'u34'\n",
      "  'u35' 'u36' 'u42' 'u43' 'u44' 'u45' 'u47' 'u49' 'u51' 'u52' 'u53' 'u56'\n",
      "  'u58' 'u59']\n",
      " ['3' '4' '5' '4' '8' '0' '8' '2' '4' '3' '1' '12' '18' '12' '4' '8' '21'\n",
      "  '7' '7' '0' '5' '2' '25' '6' '7' '1' '0' '4' '2' '2' '1' '8' '0' '15'\n",
      "  '11' '3' '8' '7']]\n"
     ]
    }
   ],
   "source": [
    "print(phq_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1f2196-63bc-446c-8c31-00226592bd5e",
   "metadata": {},
   "source": [
    "# Feature Extraction Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3788fec5-88ac-48d9-9576-9e0612cb1538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survey idenfitication \n",
    "survey = phq_final\n",
    "\n",
    "# Create the dataset\n",
    "from functions import time_difference\n",
    "from functions import kmeans_cluster\n",
    "from functions import var_trends_activity\n",
    "from functions import var_trends_audio\n",
    "from functions import var_trends_convo\n",
    "from functions import GPS_csv_reader\n",
    "from functions import entropy_fct\n",
    "from functions import wavelet_denoise\n",
    "from functions import wavelet_denoise2\n",
    "from functions import percentage_features\n",
    "from functions import haversine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebe995a",
   "metadata": {},
   "source": [
    "### Creating empty feature vectors\n",
    "- Average View \n",
    "- Location View\n",
    "- Trends View\n",
    "\n",
    "Trends view is composed of Walking, Noise, and Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f5fdd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature vector\n",
    "avg_view = []\n",
    "loc_view = []\n",
    "\n",
    "# Beginning of file Trends\n",
    "walk_f1 = open(\"python_walk.txt\", \"w\")\n",
    "aud_f2 = open(\"python_noise.txt\", \"w\")\n",
    "convo_f3 = open(\"python_convo.txt\", \"w\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bebbe05-848d-44de-a20d-614253253c48",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ae42ee7-d91b-45f0-a403-cfee2ee3ffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop to run through all subjects\n",
    "for subject in survey[0]:\n",
    "\n",
    "    ############################ Average view\n",
    "    \n",
    "    # Activity feature \n",
    "    sensor_num = 0\n",
    "    act = sensor_val (subject, sensor_num)\n",
    "    activity = sensor_val (subject, sensor_num).iloc[:,1]\n",
    "    \n",
    "    act_s =  (activity [activity==0])\n",
    "    act_w =  (activity [activity==1])\n",
    "    act_r =  (activity [activity==2]) \n",
    "    \n",
    "    # Take length between 2 - 3 seconds (sampling freq)\n",
    "    r1 = random.uniform(2, 3, len(act_s))\n",
    "    r2 = random.uniform(2, 3, len(act_w))\n",
    "    r3 = random.uniform(2, 3, len(act_r))\n",
    "    \n",
    "    avg_act_s = np.sum(r1)\n",
    "    avg_act_w = np.sum(r2)\n",
    "    avg_act_r = np.sum(r3)\n",
    "\n",
    "    \n",
    "    # Conversation \n",
    "    sensor_num = 3\n",
    "    convo = sensor_val (subject, sensor_num)\n",
    "    x, x, convo_dur = time_difference (convo)\n",
    "    \n",
    "    avg_convo_dur = convo_dur\n",
    "    avg_convo_num = len (convo)\n",
    "\n",
    "\n",
    "    # Dark features    \n",
    "    sensor_num = 4\n",
    "    dark = sensor_val (subject, sensor_num)\n",
    "    x, x, dark_dur = time_difference (dark)\n",
    "    \n",
    "    avg_dark_dur = dark_dur\n",
    "    avg_dark_num = len (dark)\n",
    "\n",
    "    \n",
    "    # Audio features \n",
    "    # The total duration when the audio is classified as quiet, noisy and voice in a day.\n",
    "    sensor_num = 1\n",
    "    audio = sensor_val (subject, sensor_num)[' audio inference'] \n",
    "\n",
    "    aud_s = (audio[audio==0]) \n",
    "    aud_v = (audio[audio==1]) \n",
    "    aud_n = (audio[audio==2]) \n",
    "\n",
    "    # Take length between 2 - 3 seconds (sampling freq)\n",
    "    r1 = random.uniform(2, 3, len(aud_s))\n",
    "    r2 = random.uniform(2, 3, len(aud_v))\n",
    "    r3 = random.uniform(2, 3, len(aud_n))\n",
    "    \n",
    "    avg_aud_s = np.sum(r1)\n",
    "    avg_aud_v = np.sum(r2)\n",
    "    avg_aud_n = np.sum(r3)\n",
    "    \n",
    "\n",
    "    # Phone Lock\n",
    "    sensor_num = 7\n",
    "    lock = sensor_val (subject, sensor_num)\n",
    "    x, x, lock_dur = time_difference (lock)\n",
    "    \n",
    "    avg_lock_dur = lock_dur\n",
    "    avg_lock_num = len (lock)\n",
    "\n",
    "\n",
    "    hold = avg_act_s, avg_act_w, avg_act_r,\\\n",
    "     avg_convo_dur,avg_convo_num,\\\n",
    "        avg_dark_dur,avg_dark_num,\\\n",
    "            avg_aud_s,avg_aud_v,avg_aud_n,\\\n",
    "                avg_lock_dur, avg_lock_num\n",
    "    avg_view.append (hold)\n",
    "\n",
    "    ############################ Trend view \n",
    "\n",
    "    # Daily trends in each day for activity\n",
    "    sensor_num = 0\n",
    "    act = sensor_val (subject, sensor_num)\n",
    "    x,x,x,daily_act_s,daily_act_w,daily_act_r = var_trends_activity (act)\n",
    "    \n",
    "    # Daily trends in each day for Noise\n",
    "    sensor_num = 1\n",
    "    audio = sensor_val (subject, sensor_num)\n",
    "    x,x,x, daily_aud_s, daily_aud_v, daily_aud_n = var_trends_audio(audio)    \n",
    "    \n",
    "    # Daily trends in each day for Conversation\n",
    "    sensor_num = 3\n",
    "    convo = sensor_val (subject, sensor_num)\n",
    "    daily_convo = var_trends_convo (convo)\n",
    "    \n",
    "    # Walking, Noise, Conversation to be placed in MATLAB\n",
    "    # Write to file Daily Activity\n",
    "    for row in daily_act_w:\n",
    "        # print (row)\n",
    "        walk_f1.write (str(row))\n",
    "        walk_f1.write ('\\n')\n",
    "    walk_f1.write('-1000\\n')\n",
    "    \n",
    "    # Write to file Daily Noise\n",
    "    for row in daily_aud_n:\n",
    "        # print (row)\n",
    "        aud_f2.write (str(row))\n",
    "        aud_f2.write ('\\n')\n",
    "    aud_f2.write('-1000\\n')\n",
    "    \n",
    "    # Write to file Daily Convo\n",
    "    for row in daily_convo:\n",
    "        # print (row)\n",
    "        convo_f3.write (str(row))\n",
    "        convo_f3.write ('\\n')\n",
    "    convo_f3.write('-1000\\n')\n",
    "\n",
    "    ############################ Location view\n",
    "\n",
    "    #  GPS feature \n",
    "    sensor_num = 5\n",
    "    gps_df = sensor_val (subject, sensor_num)\n",
    "    gps_time = GPS_csv_reader(subject,5,'time',1) \n",
    "    \n",
    "    gps_lat = gps_df.iloc [:,3]\n",
    "    gps_lon = gps_df.iloc [:,4]\n",
    "    \n",
    "\n",
    "    # GPS variance is the equation used\n",
    "    gps_lat_var = np.var (gps_lat)\n",
    "    gps_lon_var = np.var (gps_lon)\n",
    "    location_var = (gps_lat_var+gps_lon_var)\n",
    "    \n",
    "    \n",
    "    # Time in location clusters\n",
    "    # NOTE: these values are basically a percentage of times spent in\n",
    "    # each clusters. Although data was not collected continuously for all 66\n",
    "    # days, the variables will not add up to 100%\n",
    "    tc1,tc2,tc3,tc,n_clust = kmeans_cluster(gps_time,gps_lat,gps_lon, 0)\n",
    "    \n",
    "    # Entropy and Normalized entropy\n",
    "    entropy, norm_entropy = entropy_fct (n_clust,tc)\n",
    "\n",
    "    # Percentage at home and moving\n",
    "    home_d, move_p = percentage_features (gps_lat,gps_lon,gps_time,0,subject)\n",
    "    \n",
    "    # Total distance using Haversine Formula\n",
    "    # https://stackoverflow.com/questions/4913349/haversine-formula-in-python-bearing-and-distance-between-two-gps-points\n",
    "        \n",
    "    # Filtering techniques involved to avoid large differences in GPS points\n",
    "    gps_lat_red = gps_lat[gps_lat.between\\\n",
    "                  (gps_lat.quantile(.10),gps_lat.quantile(.90))]\n",
    "    gps_lon_red = gps_lon[gps_lat.between\\\n",
    "                  (gps_lat.quantile(.10),gps_lat.quantile(.90))]\n",
    "        \n",
    "    lon1 = gps_lon_red [0:-1]\n",
    "    lat1 = gps_lat_red [0:-1]\n",
    "    lon2 = gps_lon_red [1:]\n",
    "    lat2 = gps_lat_red [1:]\n",
    "    \n",
    "    # V1 - Without filtering out bad points\n",
    "    # lon1 = gps_lon [0:-1]\n",
    "    # lat1 = gps_lat [0:-1]\n",
    "    # lon2 = gps_lon [1:]\n",
    "    # lat2 = gps_lat [1:]\n",
    "        \n",
    "    tot_dist = 0\n",
    "    for i in range (len(lat1)):\n",
    "        tot_dist+=haversine(lon1.iloc[i], lat1.iloc[i],\\\n",
    "                            lon2.iloc[i], lat2.iloc[i])\n",
    "\n",
    "    hold = location_var, tc1,tc2,tc3, entropy, norm_entropy,\\\n",
    "        home_d,move_p,tot_dist\n",
    "    \n",
    "    loc_view.append (hold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1a158c0-8ea1-4958-bacc-a69a42a0ba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHQ-9 Analysis\n",
    "from functions import survey_reader\n",
    "from functions import survey_encoder_phq\n",
    "\n",
    "surv_num = 0\n",
    "phq = survey_reader (surv_num)\n",
    "dataset_score_pre = []\n",
    "dataset_score_post = []\n",
    "user_pre = []\n",
    "user_post = []\n",
    "for i in range (len(phq)):\n",
    "    user = phq.iloc [i,2:-1]\n",
    "    phq_score = np.sum (survey_encoder_phq(user))\n",
    "    if (phq.iloc[i,1]=='pre'):\n",
    "        dataset_score_pre.append(phq_score)\n",
    "        user_pre.append (phq.iloc[i,0])\n",
    "    elif (phq.iloc[i,1]=='post'):\n",
    "        dataset_score_post.append(phq_score)\n",
    "        user_post.append (phq.iloc[i,0])\n",
    "        \n",
    "phq_post = np.vstack((user_post,dataset_score_post))\n",
    "phq_pre = np.vstack((user_pre,dataset_score_pre))\n",
    "\n",
    "phq_final = phq_post\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a728ed0b",
   "metadata": {},
   "source": [
    "# Label Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0cf78553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing label\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from functions import phq_severity\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# phq_lab = label_encoder.fit_transform(phq_severity(\\\n",
    "#     np.asfarray(phq_final[1],float)))\n",
    "\n",
    "phq_lab = label_encoder.fit_transform(phq_final[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05775a88",
   "metadata": {},
   "source": [
    "# Export and save extracted features to txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f093bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHQ-9\n",
    "np.savetxt(\"phq_score.txt\", np.array (phq_lab), delimiter=',')  \n",
    "\n",
    "# Average \n",
    "np.savetxt(\"avg_view.txt\", np.array (avg_view), delimiter=',')  \n",
    "\n",
    "# Location \n",
    "np.savetxt(\"loc_view.txt\", np.array (loc_view), delimiter=',')  \n",
    "\n",
    "# Trends\n",
    "walk_f1.close()\n",
    "aud_f2.close()\n",
    "convo_f3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc38929",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
